{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff887ae-1e99-4c0f-8f59-4b3c88771b41",
   "metadata": {},
   "source": [
    "# Triton Server Image Recognition Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c6c00-3fdb-4665-9379-2a7c1f677e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tritonclient.grpc as grpcclient\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import ipyplot\n",
    "\n",
    "from utils import *\n",
    "from bbox import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd88c6-9ed7-4708-9337-fa1fe91fce78",
   "metadata": {},
   "source": [
    "# RedisDB for saving inference data\n",
    "\n",
    "## Model Overview\n",
    "```python\n",
    "class Bbox(JsonModel):\n",
    "    x1: int\n",
    "    y1: int\n",
    "    x2: int\n",
    "    y2: int\n",
    "\n",
    "\n",
    "class Face(JsonModel):\n",
    "    bbox: Bbox\n",
    "    probability: int\n",
    "    label: Optional[int] = None\n",
    "    rotation: Optional[int] = None\n",
    "    descriptors: Optional[Dict] = None\n",
    "\n",
    "\n",
    "class Model(JsonModel):\n",
    "    filename: str = Field(index=True, full_text_search=True)\n",
    "    faces: Optional[List[Face]] = None\n",
    "    channels: Optional[int] = None\n",
    "    height: Optional[int] = None\n",
    "    width: Optional[int] = None\n",
    "    portrait: Optional[int] = None\n",
    "\n",
    "    class Meta:\n",
    "        database = get_redis_connection()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21720f4-c0ad-4feb-839b-80c00f7d7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "Migrator().run()\n",
    "\n",
    "query = CONTAINER_IMAGE_FOLDER.split(\"/\")[1]\n",
    "delete = Model.find(Model.filename % query).all()\n",
    "for r in tqdm(delete, desc=\"Deleting existing keys\"):\n",
    "    try:\n",
    "        Model.delete(r.pk)\n",
    "    except Exception as e:\n",
    "        print(\"There was an exception with {}\\n{}\".format(r, e))\n",
    "\n",
    "formats = (\".jpg\", \".jpeg\", \".png\")\n",
    "filenames = index_directory(CONTAINER_IMAGE_FOLDER, formats=formats)\n",
    "\n",
    "for filename in tqdm(filenames, desc=\"Writing filenames to redis db.\"):\n",
    "    try:\n",
    "        Model(filename=filename).save()\n",
    "    except Exception as e:\n",
    "        print(\"There was an exception with {}\\n{}\".format(filename, e))\n",
    "\n",
    "models = Model.find(Model.filename % query).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d780b240-ba3f-4dc5-9185-e6cf190727b8",
   "metadata": {},
   "source": [
    "# FaceDetect Model\n",
    "\n",
    "## Model Overview <a class=\"anchor\" name=\"model_overview\"></a>\n",
    "\n",
    "The model described in this card detects one or more faces in the given image / video. Compared to the FaceirNet model, this model gives better results on RGB images and smaller faces.\n",
    "\n",
    "## Model Architecture <a class=\"anchor\" name=\"model_architecture\"></a>\n",
    "\n",
    "The model is based on NVIDIA DetectNet_v2 detector with ResNet18 as a feature extractor. This architecture, also known as GridBox object detection, uses bounding-box regression on a uniform grid on the input image. Gridbox system divides an input image into a grid which predicts four normalized bounding-box parameters (xc, yc, w, h) and confidence value per output class.\n",
    "\n",
    "The raw normalized bounding-box and confidence detections needs to be post-processed by a clustering algorithm such as DBSCAN or NMS to produce final bounding-box coordinates and category labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad37d6-ef75-42bb-a6ae-4054705dad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "facedetect_client = TritonClient(\n",
    "    model_name=FACE_DETECT_MODEL_NAME,\n",
    "    model_version=MODEL_VERSION,\n",
    "    url=TRITON_SERVER_URL,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "rmi = RedisModelIterator(models, THREAD_CHUNKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e12e4-8bd8-4045-bf80-86007a8c49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_total = (len(models) // THREAD_CHUNKS) + 1\n",
    "pbar = tqdm(\n",
    "    total=est_total,\n",
    "    desc=\"Submitting photos to {} at {}\".format(\n",
    "        FACE_DETECT_MODEL_NAME, TRITON_SERVER_URL\n",
    "    ),\n",
    ")\n",
    "\n",
    "facedetect_exceptions = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for model_batch, image_batch in rmi:\n",
    "        futures = []\n",
    "        futures.append(\n",
    "            executor.submit(facedetect_client.test_infer, model_batch, image_batch)\n",
    "        )\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            infer_results = future.result()\n",
    "\n",
    "            for infer_result in infer_results:\n",
    "                try:\n",
    "                    model = Model.get(infer_result.get_response().id)\n",
    "                    image_wise_bboxes = infer_result.as_numpy(\n",
    "                        facedetect_client.output_names[0]\n",
    "                    ).reshape(-1, 4)\n",
    "                    image_probas = infer_result.as_numpy(\n",
    "                        facedetect_client.output_names[1]\n",
    "                    ).reshape(-1, 1)\n",
    "                    h, w, c = infer_result.as_numpy(\n",
    "                        facedetect_client.output_names[2]\n",
    "                    ).squeeze()\n",
    "                    model.channels = int(c)\n",
    "                    model.height = int(h)\n",
    "                    model.width = int(w)\n",
    "                    model.portrait = int(h > w)\n",
    "                    for bbox, proba in zip(image_wise_bboxes, image_probas):\n",
    "                        model.faces = [\n",
    "                            {\n",
    "                                \"bbox\": {\n",
    "                                    \"x1\": int(round(bbox[0], 0)),\n",
    "                                    \"y1\": int(round(bbox[1], 0)),\n",
    "                                    \"x2\": int(round(bbox[2], 0)),\n",
    "                                    \"y2\": int(round(bbox[3], 0)),\n",
    "                                },\n",
    "                                \"probability\": int(round(proba[0], 0)),\n",
    "                            }\n",
    "                            for bbox, proba in zip(image_wise_bboxes, image_probas)\n",
    "                        ]\n",
    "                    model.save()\n",
    "\n",
    "                except BaseException as e:\n",
    "                    try:\n",
    "                        facedetect_exceptions.append(infer_result.message())\n",
    "                    except:\n",
    "                        facedetect_exceptions.append(e)\n",
    "\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b09e8f0-095a-4a13-890e-c4bd953da0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-query since it's all been updated.\n",
    "models = Model.find(Model.filename % query).all()\n",
    "models = [model for model in models if model.faces]\n",
    "# random.shuffle(models)\n",
    "\n",
    "for model in models:\n",
    "    try:\n",
    "        if len(model.faces) > 1:\n",
    "            print(\n",
    "                \"Looks like {} has {} faces.\\n\".format(model.filename, len(model.faces))\n",
    "            )\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    except BaseException:\n",
    "        pass\n",
    "print(model.dict())\n",
    "\n",
    "render_image(model, output_size=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead74429-4045-484a-a237-5a861cb92645",
   "metadata": {},
   "source": [
    "# Facial Landmark Estimator (FPENet) Model Card\n",
    "\n",
    "## Model Overview <a class=\"anchor\" name=\"model_overview\"></a>\n",
    "\n",
    "The FPENet model described in this card is a facial keypoints estimator network, which aims to predict the (x,y) location of keypoints for a given input face image. FPEnet is generally used in conjuction with a face detector and the output is commonly used for face alignment, head pose estimation, emotion detection, eye blink detection, gaze estimation, among others.\n",
    "\n",
    "This model predicts 68, 80 or 104 keypoints for a given face- Chin: 1-17, Eyebrows: 18-27, Nose: 28-36, Eyes: 37-48, Mouth: 49-61, Inner Lips: 62-68, Pupil: 69-76, Ears: 77-80, additional eye landmarks: 81-104. It can also handle visible or occluded flag for each keypoint. An example of the kaypoints is shown as follows:\n",
    "\n",
    "<img style=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/TLT/fpe_sample_keypoints.png\" width=\"500\"> <br>\n",
    "\n",
    "## Model Architecture <a class=\"anchor\" name=\"model_architecture\"></a>\n",
    "\n",
    "This is a classification model with a [Recombinator network](https://openaccess.thecvf.com/content_cvpr_2016/papers/Honari_Recombinator_Networks_Learning_CVPR_2016_paper.pdf) backbone. Recombinator networks are a family of CNN architectures that are suited for fine grained pixel level predictions (as oppose to image level prediction like classification). The model recombines the layer inputs such that convolutional layers in the finer branches get inputs from both coarse and fine layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44ebdb-2807-44bd-8fb9-63f0483e3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpenet_client = TritonClient(\n",
    "    model_name=FPENET_MODEL_NAME,\n",
    "    model_version=MODEL_VERSION,\n",
    "    url=TRITON_SERVER_URL,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "rmi = RedisModelIterator(models, THREAD_CHUNKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d06f1-6133-4f7b-9289-0cade8a365b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_total = (len(models) // THREAD_CHUNKS) + 1\n",
    "pbar = tqdm(\n",
    "    total=est_total,\n",
    "    desc=\"Submitting photos to {} at {}\".format(FPENET_MODEL_NAME, TRITON_SERVER_URL),\n",
    ")\n",
    "\n",
    "fpenet_exceptions = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for model_batch, image_batch in rmi:\n",
    "        futures = []\n",
    "        futures.append(\n",
    "            executor.submit(fpenet_client.test_infer, model_batch, image_batch)\n",
    "        )\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            infer_results = future.result()\n",
    "\n",
    "            for infer_result in infer_results:\n",
    "                try:\n",
    "                    request_id = infer_result.get_response().id\n",
    "                    model_pk, i = request_id.split(\"-\")\n",
    "                    model = Model.get(model_pk)\n",
    "                    image_points = infer_result.as_numpy(\n",
    "                        fpenet_client.output_names[1]\n",
    "                    ).squeeze()\n",
    "                    model.faces[int(i)].rotation = get_fpenet_rotation(image_points)\n",
    "                    model.faces[int(i)].descriptors = parse_descriptors(image_points)\n",
    "                    model.save()\n",
    "\n",
    "                except BaseException as e:\n",
    "                    try:\n",
    "                        fpenet_exceptions.append(infer_result.message())\n",
    "                    except:\n",
    "                        fpenet_exceptions.append(e)\n",
    "\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf1366-5891-49bb-8375-df040150e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = Model.find(Model.filename % query).all()\n",
    "models = [model for model in models if model.faces]\n",
    "random.shuffle(models)\n",
    "for model in models:\n",
    "    if len(model.faces) > 1:\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"Looks like {} has {} faces.\\n\".format(model.filename, len(model.faces)))\n",
    "original_crops = crop_and_rotate_clip(model)\n",
    "# new_rotated_bbox is the function which applies the appropriate tranformation of the BBOX relative to the calculated rotation in degrees.\n",
    "model_copy = model.copy(deep=True)\n",
    "model_copy = new_rotated_bbox(model_copy)\n",
    "modified_crops = crop_and_rotate_clip(model_copy, rotate=True)\n",
    "\n",
    "\n",
    "ipyplot.plot_images(\n",
    "    original_crops,\n",
    "    labels=[\n",
    "        \"bbox #{}: 0\\N{DEGREE SIGN} rotation\".format(i)\n",
    "        for i, face in enumerate(model.faces)\n",
    "    ],\n",
    "    show_url=False,\n",
    ")\n",
    "ipyplot.plot_images(\n",
    "    modified_crops,\n",
    "    labels=[\n",
    "        \"bbox #{}: {}\\N{DEGREE SIGN} rotation\".format(i, face.rotation)\n",
    "        for i, face in enumerate(model_copy.faces)\n",
    "    ],\n",
    "    show_url=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518b91b-0562-4730-ab7f-ff14d9ce35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_image(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4da92-ede8-4ea6-8f42-28466eed9b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
