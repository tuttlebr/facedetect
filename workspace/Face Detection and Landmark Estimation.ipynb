{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38cbc9e-b763-4d18-9b58-501bc96e27e4",
   "metadata": {},
   "source": [
    "# Face Detection and Landmark Estimation Workflow\n",
    "\n",
    "## NVIDIA Data Loading Library (DALI) \n",
    "\n",
    "DALI is a collection of highly optimized building blocks and an execution engine that accelerates the data pipeline for computer vision and audio deep learning applications.\n",
    "\n",
    "Input and augmentation pipelines provided by Deep Learning frameworks fit typically into one of two categories:\n",
    "\n",
    "- fast, but inflexible - written in C++, they are exposed as a single monolithic Python object with very specific set and ordering of operations it provides\n",
    "- slow, but flexible - set of building blocks written in either C++ or Python, that can be used to compose arbitrary data pipelines that end up being slow. One of the biggest overheads for this type of data pipelines is Global Interpreter Lock (GIL) in Python. This forces developers to use multiprocessing, complicating the design of efficient input pipelines.\n",
    "\n",
    "DALI stands out by providing both performance and flexibility of accelerating different data pipelines. It achieves that by exposing optimized building blocks which are executed using simple and efficient engine, and enabling offloading of operations to GPU (thus enabling scaling to multi-GPU systems).\n",
    "\n",
    "It is a single library, that can be easily integrated into different deep learning training and inference applications.\n",
    "\n",
    "DALI offers ease-of-use and flexibility across GPU enabled systems with direct framework plugins, multiple input data formats, and configurable graphs. DALI can help achieve overall speedup on deep learning workflows that are bottlenecked on I/O pipelines due to the limitations of CPU cycles. Typically, systems with high GPU to CPU ratio are constrained on the host CPU, thereby under-utilizing the available GPU compute capabilities. DALI significantly accelerates input processing on such dense GPU configurations to achieve the overall throughput.\n",
    "\n",
    "___\n",
    "## FaceDetect Model\n",
    "\n",
    "### Model Overview <a class=\"anchor\" name=\"model_overview\"></a>\n",
    "\n",
    "The model described in this card detects one or more faces in the given image / video. Compared to the FaceirNet model, this model gives better results on RGB images and smaller faces.\n",
    "\n",
    "### Model Architecture <a class=\"anchor\" name=\"model_architecture\"></a>\n",
    "\n",
    "The model is based on NVIDIA DetectNet_v2 detector with ResNet18 as a feature extractor. This architecture, also known as GridBox object detection, uses bounding-box regression on a uniform grid on the input image. Gridbox system divides an input image into a grid which predicts four normalized bounding-box parameters (xc, yc, w, h) and confidence value per output class.\n",
    "\n",
    "The raw normalized bounding-box and confidence detections needs to be post-processed by a clustering algorithm such as DBSCAN or NMS to produce final bounding-box coordinates and category labels.\n",
    "\n",
    "___\n",
    "## Facial Landmark Estimator (FPENet) Model Card\n",
    "\n",
    "### Model Overview <a class=\"anchor\" name=\"model_overview\"></a>\n",
    "\n",
    "The FPENet model described in this card is a facial keypoints estimator network, which aims to predict the (x,y) location of keypoints for a given input face image. FPEnet is generally used in conjuction with a face detector and the output is commonly used for face alignment, head pose estimation, emotion detection, eye blink detection, gaze estimation, among others.\n",
    "\n",
    "This model predicts 68, 80 or 104 keypoints for a given face- Chin: 1-17, Eyebrows: 18-27, Nose: 28-36, Eyes: 37-48, Mouth: 49-61, Inner Lips: 62-68, Pupil: 69-76, Ears: 77-80, additional eye landmarks: 81-104. It can also handle visible or occluded flag for each keypoint. An example of the keypoints is shown as follows:\n",
    "\n",
    "<img style=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/TLT/fpe_sample_keypoints.png\" width=\"500\"> <br>\n",
    "\n",
    "### Model Architecture <a class=\"anchor\" name=\"model_architecture\"></a>\n",
    "\n",
    "This is a classification model with a [Recombinator network](https://openaccess.thecvf.com/content_cvpr_2016/papers/Honari_Recombinator_Networks_Learning_CVPR_2016_paper.pdf) backbone. Recombinator networks are a family of CNN architectures that are suited for fine grained pixel level predictions (as oppose to image level prediction like classification). The model recombines the layer inputs such that convolutional layers in the finer branches get inputs from both coarse and fine layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab96f64-9e56-430a-b1b7-8dbe78a696f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from numpy import int32, float32, array\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator, LastBatchPolicy\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "from preprocessing import coco_pipeline, facedetect_pipeline\n",
    "from schema import COCOModel, Migrator\n",
    "from utils import index_directory\n",
    "from viz import show\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508227f4-3475-4238-8218-19638ff5d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size = 1\n",
    "num_threads = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1c07e-0a99-42d0-b5f6-6c683e0baf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/volume1/brandon/pictures\"\n",
    "formats = (\".jpg\", \".jpeg\", \".png\")\n",
    "filenames = index_directory(root_dir, formats=formats, random_order=True)\n",
    "print(\"{:,} files.\".format(len(filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2ed08-5063-4179-9e40-258c37fb01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = os.getenv(\"TRITON_SERVER_URL\")\n",
    "client = grpcclient.InferenceServerClient(url=url, verbose=False)\n",
    "facedetect_outputs = [\n",
    "    grpcclient.InferRequestedOutput(\"true_boxes\"),\n",
    "    grpcclient.InferRequestedOutput(\"true_proba\"),\n",
    "    grpcclient.InferRequestedOutput(\"true_image_size\"),\n",
    "]\n",
    "\n",
    "fpenet_outputs = [\n",
    "    grpcclient.InferRequestedOutput(\"conv_keypoints_m80\"),\n",
    "    grpcclient.InferRequestedOutput(\"softargmax\"),\n",
    "    grpcclient.InferRequestedOutput(\"softargmax:1\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ec547-fc1a-40c1-bbe4-e191f0e560fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_annotations_file = \"coco/instances.json\"\n",
    "face_category_id = 0\n",
    "data = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": [{\"supercategory\": \"Face\", \"id\": face_category_id, \"name\": \"Face\"}],\n",
    "}\n",
    "\n",
    "\n",
    "facedetect_0_pipe = facedetect_pipeline(\n",
    "    filenames=filenames,\n",
    "    device_id=0,\n",
    "    shard_id=0,\n",
    "    num_shards=1,\n",
    "    batch_size=max_batch_size,\n",
    "    num_threads=num_threads,\n",
    ")\n",
    "facedetect_0_pipe.build()\n",
    "\n",
    "# device_1_pipe = encoded_pipeline(\n",
    "#     device_id=1,\n",
    "#     shard_id=1,\n",
    "#     num_shards=2,\n",
    "#     batch_size=max_batch_size,\n",
    "#     num_threads=num_threads,\n",
    "# )\n",
    "# device_1_pipe.build()\n",
    "\n",
    "facedetect_shard_pipelines = [facedetect_0_pipe]\n",
    "# facedetect_shard_pipelines = [facedetect_0_pipe, facedetect_1_pipe]\n",
    "\n",
    "\n",
    "loader = DALIGenericIterator(\n",
    "    facedetect_shard_pipelines,\n",
    "    [\"shapes\", \"images\", \"encoded\"],\n",
    "    reader_name=\"Encoder\",\n",
    "    last_batch_policy=LastBatchPolicy.PARTIAL,\n",
    ")\n",
    "\n",
    "loader_len = int(math.ceil(loader._size / max_batch_size))\n",
    "pbar = tqdm(\n",
    "    total=loader_len,\n",
    "    desc=\"Calculating image encoding\",\n",
    ")\n",
    "\n",
    "fidx = 0\n",
    "for n, b in enumerate(loader):\n",
    "    for shard in b:\n",
    "        shapes = shard[\"shapes\"]\n",
    "        images = shard[\"images\"]\n",
    "        encoded = shard[\"encoded\"]\n",
    "        np_images = images.cpu().numpy()\n",
    "        np_shapes = shapes.cpu().numpy()\n",
    "        np_encoded = encoded.cpu().numpy()\n",
    "        facedetect_inputs = [\n",
    "            grpcclient.InferInput(\"input_1\", np_images.shape, \"FP32\"),\n",
    "            grpcclient.InferInput(\"true_image_size\", np_shapes.shape, \"INT64\"),\n",
    "        ]\n",
    "        facedetect_inputs[0].set_data_from_numpy(np_images)\n",
    "        facedetect_inputs[1].set_data_from_numpy(np_shapes)\n",
    "        facedetect_infer_result = client.infer(\n",
    "            \"facenet_resized\",\n",
    "            facedetect_inputs,\n",
    "            model_version=\"1\",\n",
    "            outputs=facedetect_outputs,\n",
    "        )\n",
    "        true_boxes = facedetect_infer_result.as_numpy(facedetect_outputs[0].name())\n",
    "        true_proba = facedetect_infer_result.as_numpy(facedetect_outputs[1].name())\n",
    "\n",
    "        for idx, mb in enumerate(shapes):\n",
    "            try:\n",
    "                bbox = [i.tolist() for i in eval(true_boxes[idx].decode())]\n",
    "            except:\n",
    "                bbox = true_boxes[idx].tolist()\n",
    "\n",
    "            data[\"images\"].append(\n",
    "                {\n",
    "                    \"id\": fidx,\n",
    "                    \"height\": int(mb[0]),\n",
    "                    \"width\": int(mb[1]),\n",
    "                    \"channels\": int(mb[2]),\n",
    "                    \"file_name\": filenames[fidx],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            for b in bbox:\n",
    "                b = array((eval(b[0]), eval(b[1]), eval(b[2]), eval(b[3])))\n",
    "                np_b = array(b).astype(int32).reshape(1, -1)\n",
    "                fpenet_inputs = [\n",
    "                    grpcclient.InferInput(\"raw_image_data\", np_encoded.shape, \"UINT8\"),\n",
    "                    grpcclient.InferInput(\"true_boxes\", np_b.shape, \"INT32\"),\n",
    "                ]\n",
    "                fpenet_inputs[0].set_data_from_numpy(np_encoded)\n",
    "                fpenet_inputs[1].set_data_from_numpy(np_b)\n",
    "\n",
    "                fpenet_infer_result = client.infer(\n",
    "                    \"fpenet_ensemble\",\n",
    "                    fpenet_inputs,\n",
    "                    model_version=\"1\",\n",
    "                    outputs=fpenet_outputs,\n",
    "                )\n",
    "                segmentation = fpenet_infer_result.as_numpy(fpenet_outputs[1].name())\n",
    "                data[\"annotations\"].append(\n",
    "                    {\n",
    "                        \"image_id\": fidx,\n",
    "                        \"bbox\": b.tolist(),\n",
    "                        \"category_id\": face_category_id,\n",
    "                        \"segmentation\": segmentation[0].tolist(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            fidx += 1\n",
    "\n",
    "    pbar.update()\n",
    "\n",
    "with open(coco_annotations_file, \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f76b3-b126-4df2-b87c-3d5f56df509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Migrator().run()\n",
    "model = COCOModel(\n",
    "    images=data[\"images\"],\n",
    "    annotations=data[\"annotations\"],\n",
    "    categories=data[\"categories\"],\n",
    ")\n",
    "_ = model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42a45c-0bb5-48f7-899b-79630338b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_annotations_file = \"coco/instances.json\"\n",
    "with open(coco_annotations_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "coco_pipe = coco_pipeline(\n",
    "    coco_annotations_file=coco_annotations_file,\n",
    "    batch_size=4,\n",
    "    num_threads=num_threads,\n",
    "    device_id=0,\n",
    ")\n",
    "coco_pipe.build()\n",
    "outputs = coco_pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5d533-9c5f-4b24-b751-54fd0e2b0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(outputs, dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fc7df-ba01-4791-a35f-aa7e9fa657ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
